{
 "cells": [
  {
   "cell_type": "code",
   "id": "1c77b0a89fdbcd37",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-07T13:14:01.976467Z",
     "start_time": "2025-02-07T13:14:01.964012Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from litedl.models import FeedForwardNeuralNetwork\n",
    "from litedl.layers import Affine, SoftmaxWithLoss, ReLU\n",
    "from litedl.optimizers import SGD\n",
    "from litedl.utils import data_split, OneHotEncoder\n",
    "\n",
    "# 학습 하이퍼 파라미터 설정\n",
    "epochs = 300\n",
    "batch_size = 20\n",
    "\n",
    "# 붓꽃 데이터셋 불러오기\n",
    "dataset = pd.read_csv('dataset/Iris.csv')\n",
    "dataset.drop('Id', axis=1, inplace=True)\n",
    "\n",
    "# 피처와 라벨 데이터 나누기\n",
    "features = dataset.iloc[:, :-1].values\n",
    "labels = dataset.iloc[:, -1].values.reshape(-1, 1)\n",
    "\n",
    "# 라벨 데이터를 원 핫 인코딩으로 표현하고 섞기\n",
    "encoder = OneHotEncoder()\n",
    "encoder.fit(labels)\n",
    "labels = encoder.transform(labels)\n",
    "\n",
    "# 데이터 섞기\n",
    "indices = np.arange(features.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "features = features[indices]\n",
    "labels = labels[indices]\n",
    "\n",
    "# 학습용 데이터와 테스트용 데이터로 나누기\n",
    "train_x, test_x = data_split(features, 0.2)\n",
    "train_y, test_y = data_split(labels, 0.2)"
   ],
   "outputs": [],
   "execution_count": 124
  },
  {
   "cell_type": "code",
   "id": "dca0544489e62623",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-07T13:14:02.011747Z",
     "start_time": "2025-02-07T13:14:01.976467Z"
    }
   },
   "source": [
    "# 모델과 계층 선언\n",
    "model = FeedForwardNeuralNetwork()\n",
    "affine1 = Affine(input_size=4, output_size=5)\n",
    "relu = ReLU()\n",
    "affine2 = Affine(input_size=5, output_size=3)\n",
    "loss_fn = SoftmaxWithLoss()\n",
    "optimizer = SGD(lr=0.1)\n",
    "\n",
    "# 계층을 쌓아 모델을 구축\n",
    "model.add_layer(affine1)\n",
    "model.add_layer(relu)\n",
    "model.add_layer(affine2)\n",
    "model.add_loss_layer(loss_fn)\n",
    "\n",
    "# 모델 학습\n",
    "for epoch in range(epochs):\n",
    "    loss = model.forward(train_x, train_y)\n",
    "    model.backward()\n",
    "    model.step(optimizer=optimizer)\n",
    "    print(f'epoch {epoch} loss: {loss}')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 loss: 1.6787952678387215\n",
      "epoch 1 loss: 1.5597693177135674\n",
      "epoch 2 loss: 1.4304555066214022\n",
      "epoch 3 loss: 1.2868954686599616\n",
      "epoch 4 loss: 1.1794520394093218\n",
      "epoch 5 loss: 1.1387083645608342\n",
      "epoch 6 loss: 1.1274214276554135\n",
      "epoch 7 loss: 1.121343591551287\n",
      "epoch 8 loss: 1.115974587298461\n",
      "epoch 9 loss: 1.1106066887659707\n",
      "epoch 10 loss: 1.105072727371962\n",
      "epoch 11 loss: 1.0992993956055719\n",
      "epoch 12 loss: 1.09322926462943\n",
      "epoch 13 loss: 1.0868032818298174\n",
      "epoch 14 loss: 1.0799551241516985\n",
      "epoch 15 loss: 1.072608515056886\n",
      "epoch 16 loss: 1.0646751053768833\n",
      "epoch 17 loss: 1.0560524004696288\n",
      "epoch 18 loss: 1.0466465615079577\n",
      "epoch 19 loss: 1.0366480826125775\n",
      "epoch 20 loss: 1.0266554306272686\n",
      "epoch 21 loss: 1.0162518089833548\n",
      "epoch 22 loss: 1.0052863681772368\n",
      "epoch 23 loss: 0.9937332929187086\n",
      "epoch 24 loss: 0.981573818361917\n",
      "epoch 25 loss: 0.9687606633946093\n",
      "epoch 26 loss: 0.955256544707801\n",
      "epoch 27 loss: 0.9409959062950299\n",
      "epoch 28 loss: 0.9258498145848133\n",
      "epoch 29 loss: 0.9097146980759141\n",
      "epoch 30 loss: 0.8924228410796687\n",
      "epoch 31 loss: 0.8737986135099345\n",
      "epoch 32 loss: 0.8537173650077823\n",
      "epoch 33 loss: 0.8321148571122137\n",
      "epoch 34 loss: 0.8090683284106684\n",
      "epoch 35 loss: 0.7848570332907249\n",
      "epoch 36 loss: 0.760028242677432\n",
      "epoch 37 loss: 0.7350728383588355\n",
      "epoch 38 loss: 0.710549200934624\n",
      "epoch 39 loss: 0.6870273294356299\n",
      "epoch 40 loss: 0.6647664991892694\n",
      "epoch 41 loss: 0.6438959453373485\n",
      "epoch 42 loss: 0.6245100431488718\n",
      "epoch 43 loss: 0.6063862704039723\n",
      "epoch 44 loss: 0.5893654799771324\n",
      "epoch 45 loss: 0.5736721542460825\n",
      "epoch 46 loss: 0.5590189044901983\n",
      "epoch 47 loss: 0.5454842191224084\n",
      "epoch 48 loss: 0.5329146784812366\n",
      "epoch 49 loss: 0.5209571887170925\n",
      "epoch 50 loss: 0.5095201257107433\n",
      "epoch 51 loss: 0.4986339184510328\n",
      "epoch 52 loss: 0.48805592016575666\n",
      "epoch 53 loss: 0.47762833012984024\n",
      "epoch 54 loss: 0.4678287787898757\n",
      "epoch 55 loss: 0.45892612309693054\n",
      "epoch 56 loss: 0.4508973198768438\n",
      "epoch 57 loss: 0.44361356645798267\n",
      "epoch 58 loss: 0.43699467756163096\n",
      "epoch 59 loss: 0.43100727873839717\n",
      "epoch 60 loss: 0.4254035591450568\n",
      "epoch 61 loss: 0.4201521537222487\n",
      "epoch 62 loss: 0.41523788779680254\n",
      "epoch 63 loss: 0.41054291406890303\n",
      "epoch 64 loss: 0.4060307200335095\n",
      "epoch 65 loss: 0.4016682437948823\n",
      "epoch 66 loss: 0.39743560042905884\n",
      "epoch 67 loss: 0.39330590258222664\n",
      "epoch 68 loss: 0.3892767729243495\n",
      "epoch 69 loss: 0.3853301526776884\n",
      "epoch 70 loss: 0.38150127514226634\n",
      "epoch 71 loss: 0.37774222466896806\n",
      "epoch 72 loss: 0.37405208562342396\n",
      "epoch 73 loss: 0.37042362417461816\n",
      "epoch 74 loss: 0.3668485137803949\n",
      "epoch 75 loss: 0.3633225010879765\n",
      "epoch 76 loss: 0.35984330219072846\n",
      "epoch 77 loss: 0.356400504394108\n",
      "epoch 78 loss: 0.3530140075617766\n",
      "epoch 79 loss: 0.3496310871167348\n",
      "epoch 80 loss: 0.3462884946679204\n",
      "epoch 81 loss: 0.34298356808581604\n",
      "epoch 82 loss: 0.3396974769927826\n",
      "epoch 83 loss: 0.3364368186699746\n",
      "epoch 84 loss: 0.33318352052233047\n",
      "epoch 85 loss: 0.3299497599707134\n",
      "epoch 86 loss: 0.3267332052847475\n",
      "epoch 87 loss: 0.3235385575223288\n",
      "epoch 88 loss: 0.3203617710360553\n",
      "epoch 89 loss: 0.317203600369189\n",
      "epoch 90 loss: 0.3140629267037051\n",
      "epoch 91 loss: 0.3109407513097293\n",
      "epoch 92 loss: 0.3078331727871194\n",
      "epoch 93 loss: 0.30474189741196367\n",
      "epoch 94 loss: 0.30166641748971884\n",
      "epoch 95 loss: 0.2986065745002608\n",
      "epoch 96 loss: 0.29556327233040586\n",
      "epoch 97 loss: 0.29253599389185375\n",
      "epoch 98 loss: 0.2895249914598459\n",
      "epoch 99 loss: 0.2865325286317194\n",
      "epoch 100 loss: 0.28355797649930964\n",
      "epoch 101 loss: 0.28060295430503274\n",
      "epoch 102 loss: 0.2776668838716256\n",
      "epoch 103 loss: 0.2747501955314365\n",
      "epoch 104 loss: 0.2718551459496161\n",
      "epoch 105 loss: 0.26898773405548754\n",
      "epoch 106 loss: 0.266146350543274\n",
      "epoch 107 loss: 0.2633386441531225\n",
      "epoch 108 loss: 0.2605740738012478\n",
      "epoch 109 loss: 0.25786853513133756\n",
      "epoch 110 loss: 0.25526162525762086\n",
      "epoch 111 loss: 0.25282507435206547\n",
      "epoch 112 loss: 0.25070327620206057\n",
      "epoch 113 loss: 0.24927122928886603\n",
      "epoch 114 loss: 0.24908686969881313\n",
      "epoch 115 loss: 0.25227937812960666\n",
      "epoch 116 loss: 0.2619385887127273\n",
      "epoch 117 loss: 0.2911834411366592\n",
      "epoch 118 loss: 0.3498661501506603\n",
      "epoch 119 loss: 0.48496866735840827\n",
      "epoch 120 loss: 0.45944622797023\n",
      "epoch 121 loss: 0.585493291913095\n",
      "epoch 122 loss: 0.2755063619178939\n",
      "epoch 123 loss: 0.314282876855532\n",
      "epoch 124 loss: 0.32079934818025674\n",
      "epoch 125 loss: 0.3892927656137472\n",
      "epoch 126 loss: 0.3513256705538094\n",
      "epoch 127 loss: 0.4252146812654845\n",
      "epoch 128 loss: 0.3258025393204362\n",
      "epoch 129 loss: 0.3837950775740533\n",
      "epoch 130 loss: 0.3165326742817323\n",
      "epoch 131 loss: 0.36919862235066325\n",
      "epoch 132 loss: 0.3218014271738736\n",
      "epoch 133 loss: 0.3745424509851509\n",
      "epoch 134 loss: 0.3221616213922001\n",
      "epoch 135 loss: 0.3724953389147364\n",
      "epoch 136 loss: 0.31859257487035403\n",
      "epoch 137 loss: 0.36556303411866076\n",
      "epoch 138 loss: 0.31353578168733887\n",
      "epoch 139 loss: 0.3571512424659264\n",
      "epoch 140 loss: 0.31931339341827086\n",
      "epoch 141 loss: 0.3629416544149561\n",
      "epoch 142 loss: 0.31968558743730285\n",
      "epoch 143 loss: 0.3609162392462473\n",
      "epoch 144 loss: 0.3161276135907354\n",
      "epoch 145 loss: 0.35415744013640654\n",
      "epoch 146 loss: 0.31128849528711133\n",
      "epoch 147 loss: 0.3464785933254286\n",
      "epoch 148 loss: 0.3033886470381249\n",
      "epoch 149 loss: 0.3356077103716094\n",
      "epoch 150 loss: 0.3046163512457703\n",
      "epoch 151 loss: 0.3368215122568497\n",
      "epoch 152 loss: 0.30315100326589756\n",
      "epoch 153 loss: 0.33429564869384243\n",
      "epoch 154 loss: 0.3005515391830985\n",
      "epoch 155 loss: 0.3304233507430657\n",
      "epoch 156 loss: 0.29737399121618985\n",
      "epoch 157 loss: 0.32602395455089855\n",
      "epoch 158 loss: 0.2939926853507515\n",
      "epoch 159 loss: 0.32161502619431936\n",
      "epoch 160 loss: 0.28983413701177496\n",
      "epoch 161 loss: 0.31643143927211786\n",
      "epoch 162 loss: 0.286235432641717\n",
      "epoch 163 loss: 0.31219845752004294\n",
      "epoch 164 loss: 0.283203769227967\n",
      "epoch 165 loss: 0.3088210364952728\n",
      "epoch 166 loss: 0.28056037517667354\n",
      "epoch 167 loss: 0.306002764234057\n",
      "epoch 168 loss: 0.27745350308099576\n",
      "epoch 169 loss: 0.3025834833225228\n",
      "epoch 170 loss: 0.27476430597973\n",
      "epoch 171 loss: 0.2997505915073708\n",
      "epoch 172 loss: 0.2723966107861487\n",
      "epoch 173 loss: 0.29733010196769827\n",
      "epoch 174 loss: 0.27007904065164917\n",
      "epoch 175 loss: 0.29496820614936364\n",
      "epoch 176 loss: 0.2678042872123358\n",
      "epoch 177 loss: 0.29264800405661145\n",
      "epoch 178 loss: 0.2647300259814197\n",
      "epoch 179 loss: 0.2892508125622282\n",
      "epoch 180 loss: 0.26175519406009895\n",
      "epoch 181 loss: 0.2860696230543577\n",
      "epoch 182 loss: 0.2591264533738252\n",
      "epoch 183 loss: 0.2833703724627119\n",
      "epoch 184 loss: 0.256600428897953\n",
      "epoch 185 loss: 0.28082740775501464\n",
      "epoch 186 loss: 0.2543674871630955\n",
      "epoch 187 loss: 0.2786441826050414\n",
      "epoch 188 loss: 0.2522905160704209\n",
      "epoch 189 loss: 0.2766209300678522\n",
      "epoch 190 loss: 0.25025177293133355\n",
      "epoch 191 loss: 0.2745973975195175\n",
      "epoch 192 loss: 0.24816901672351566\n",
      "epoch 193 loss: 0.2724699547235864\n",
      "epoch 194 loss: 0.24599879876704786\n",
      "epoch 195 loss: 0.2701915148978625\n",
      "epoch 196 loss: 0.24373105826698696\n",
      "epoch 197 loss: 0.26776045077820154\n",
      "epoch 198 loss: 0.2413791662151535\n",
      "epoch 199 loss: 0.26520459102267213\n",
      "epoch 200 loss: 0.2389690117844324\n",
      "epoch 201 loss: 0.26256850810585847\n",
      "epoch 202 loss: 0.2364055139876374\n",
      "epoch 203 loss: 0.25974569237853273\n",
      "epoch 204 loss: 0.23384097882166816\n",
      "epoch 205 loss: 0.2569391471451093\n",
      "epoch 206 loss: 0.23094873022455573\n",
      "epoch 207 loss: 0.2536687630216199\n",
      "epoch 208 loss: 0.2282226462009576\n",
      "epoch 209 loss: 0.25064997822044394\n",
      "epoch 210 loss: 0.22568715378825907\n",
      "epoch 211 loss: 0.24789709050804282\n",
      "epoch 212 loss: 0.2233308003393951\n",
      "epoch 213 loss: 0.24537895049320171\n",
      "epoch 214 loss: 0.22111697129760924\n",
      "epoch 215 loss: 0.24303258451605203\n",
      "epoch 216 loss: 0.21899674029470792\n",
      "epoch 217 loss: 0.2407831823058898\n",
      "epoch 218 loss: 0.21691918351689216\n",
      "epoch 219 loss: 0.238557703362008\n",
      "epoch 220 loss: 0.21483872486693792\n",
      "epoch 221 loss: 0.23629404018859587\n",
      "epoch 222 loss: 0.21271986237521517\n",
      "epoch 223 loss: 0.23394667509039416\n",
      "epoch 224 loss: 0.21053948636743663\n",
      "epoch 225 loss: 0.2314889055699316\n",
      "epoch 226 loss: 0.2082880096547783\n",
      "epoch 227 loss: 0.2284646729013572\n",
      "epoch 228 loss: 0.20574748822416425\n",
      "epoch 229 loss: 0.2255421519116256\n",
      "epoch 230 loss: 0.20282691738852165\n",
      "epoch 231 loss: 0.2220629091759131\n",
      "epoch 232 loss: 0.19998039035206056\n",
      "epoch 233 loss: 0.2187106098174095\n",
      "epoch 234 loss: 0.19725631653127934\n",
      "epoch 235 loss: 0.21554314522369597\n",
      "epoch 236 loss: 0.19468094131434105\n",
      "epoch 237 loss: 0.2125870519512718\n",
      "epoch 238 loss: 0.19226063028494908\n",
      "epoch 239 loss: 0.2098445628641382\n",
      "epoch 240 loss: 0.1895680700619327\n",
      "epoch 241 loss: 0.20668287188272216\n",
      "epoch 242 loss: 0.18657172342575765\n",
      "epoch 243 loss: 0.20259855267903457\n",
      "epoch 244 loss: 0.18239851185456507\n",
      "epoch 245 loss: 0.19740047495781857\n",
      "epoch 246 loss: 0.1780574037861389\n",
      "epoch 247 loss: 0.19203240798985868\n",
      "epoch 248 loss: 0.17396611150366495\n",
      "epoch 249 loss: 0.1870838065182779\n",
      "epoch 250 loss: 0.1702403103806693\n",
      "epoch 251 loss: 0.18268674874637966\n",
      "epoch 252 loss: 0.1669501547986552\n",
      "epoch 253 loss: 0.17891601644393518\n",
      "epoch 254 loss: 0.1636422028552103\n",
      "epoch 255 loss: 0.17509829010394115\n",
      "epoch 256 loss: 0.16073946638934236\n",
      "epoch 257 loss: 0.1718449370413931\n",
      "epoch 258 loss: 0.15803772236546793\n",
      "epoch 259 loss: 0.16884793459482536\n",
      "epoch 260 loss: 0.15542628161501693\n",
      "epoch 261 loss: 0.16594823087962446\n",
      "epoch 262 loss: 0.15301111117675384\n",
      "epoch 263 loss: 0.1633005733969205\n",
      "epoch 264 loss: 0.150743123210123\n",
      "epoch 265 loss: 0.16082984536968195\n",
      "epoch 266 loss: 0.14874965139611895\n",
      "epoch 267 loss: 0.1587231643429495\n",
      "epoch 268 loss: 0.14673126944225795\n",
      "epoch 269 loss: 0.15654062113306294\n",
      "epoch 270 loss: 0.1449176111741064\n",
      "epoch 271 loss: 0.1546231091566165\n",
      "epoch 272 loss: 0.14330287505413106\n",
      "epoch 273 loss: 0.15296026804634405\n",
      "epoch 274 loss: 0.1418772629950703\n",
      "epoch 275 loss: 0.1515368444729064\n",
      "epoch 276 loss: 0.14062749350924852\n",
      "epoch 277 loss: 0.1503336194627868\n",
      "epoch 278 loss: 0.1394081389797923\n",
      "epoch 279 loss: 0.14913720916088985\n",
      "epoch 280 loss: 0.13818328881213418\n",
      "epoch 281 loss: 0.14789907132819677\n",
      "epoch 282 loss: 0.1370233340965065\n",
      "epoch 283 loss: 0.14672779011322432\n",
      "epoch 284 loss: 0.13590586965050294\n",
      "epoch 285 loss: 0.14559211325983437\n",
      "epoch 286 loss: 0.13480610487149483\n",
      "epoch 287 loss: 0.14445764212570092\n",
      "epoch 288 loss: 0.13369701820932536\n",
      "epoch 289 loss: 0.1432871259661321\n",
      "epoch 290 loss: 0.1325496754278607\n",
      "epoch 291 loss: 0.14204103833407375\n",
      "epoch 292 loss: 0.13133379373521184\n",
      "epoch 293 loss: 0.14067855217529743\n",
      "epoch 294 loss: 0.13001863646326015\n",
      "epoch 295 loss: 0.13915902520231574\n",
      "epoch 296 loss: 0.12857431226411967\n",
      "epoch 297 loss: 0.13744407771908768\n",
      "epoch 298 loss: 0.12697352024749867\n",
      "epoch 299 loss: 0.13550027882201554\n"
     ]
    }
   ],
   "execution_count": 125
  },
  {
   "cell_type": "code",
   "id": "32019f4a312570c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-07T13:14:02.018645Z",
     "start_time": "2025-02-07T13:14:02.011747Z"
    }
   },
   "source": [
    "accuracy = np.sum(np.argmax(model.predict(test_x), axis=1) == np.argmax(test_y, axis=1)) / test_x.shape[0]\n",
    "\n",
    "print(f'Accuracy: {accuracy}')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9\n"
     ]
    }
   ],
   "execution_count": 126
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
