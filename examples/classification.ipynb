{
 "cells": [
  {
   "cell_type": "code",
   "id": "1c77b0a89fdbcd37",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-07T13:37:58.884771Z",
     "start_time": "2025-02-07T13:37:58.871826Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from litedl.models import Sequential\n",
    "from litedl.layers import Affine, SoftmaxWithLoss, ReLU\n",
    "from litedl.optimizers import SGD\n",
    "from litedl.utils import data_split, OneHotEncoder\n",
    "\n",
    "# 학습 하이퍼 파라미터 설정\n",
    "epochs = 300\n",
    "batch_size = 20\n",
    "\n",
    "# 붓꽃 데이터셋 불러오기\n",
    "dataset = pd.read_csv('dataset/Iris.csv')\n",
    "dataset.drop('Id', axis=1, inplace=True)\n",
    "\n",
    "# 피처와 라벨 데이터 나누기\n",
    "features = dataset.iloc[:, :-1].values\n",
    "labels = dataset.iloc[:, -1].values.reshape(-1, 1)\n",
    "\n",
    "# 라벨 데이터를 원 핫 인코딩으로 표현하고 섞기\n",
    "encoder = OneHotEncoder()\n",
    "encoder.fit(labels)\n",
    "labels = encoder.transform(labels)\n",
    "\n",
    "# 데이터 섞기\n",
    "indices = np.arange(features.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "features = features[indices]\n",
    "labels = labels[indices]\n",
    "\n",
    "# 학습용 데이터와 테스트용 데이터로 나누기\n",
    "train_x, test_x = data_split(features, 0.2)\n",
    "train_y, test_y = data_split(labels, 0.2)"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "dca0544489e62623",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-07T13:37:58.924329Z",
     "start_time": "2025-02-07T13:37:58.884771Z"
    }
   },
   "source": [
    "# 모델과 계층 선언\n",
    "model = Sequential()\n",
    "affine1 = Affine(input_size=4, output_size=5)\n",
    "relu = ReLU()\n",
    "affine2 = Affine(input_size=5, output_size=3)\n",
    "loss_fn = SoftmaxWithLoss()\n",
    "optimizer = SGD(lr=0.1)\n",
    "\n",
    "# 계층을 쌓아 모델을 구축\n",
    "model.add_layer(affine1)\n",
    "model.add_layer(relu)\n",
    "model.add_layer(affine2)\n",
    "model.add_loss_layer(loss_fn)\n",
    "\n",
    "# 모델 학습\n",
    "for epoch in range(epochs):\n",
    "    loss = model.forward(train_x, train_y)\n",
    "    model.backward()\n",
    "    model.step(optimizer=optimizer)\n",
    "    print(f'epoch {epoch} loss: {loss}')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 loss: 1.4037853723007074\n",
      "epoch 1 loss: 1.2958004228761022\n",
      "epoch 2 loss: 1.2211047910942174\n",
      "epoch 3 loss: 1.1682042773085162\n",
      "epoch 4 loss: 1.1338195880579103\n",
      "epoch 5 loss: 1.1120328143845777\n",
      "epoch 6 loss: 1.096037530570697\n",
      "epoch 7 loss: 1.0812963158537883\n",
      "epoch 8 loss: 1.0658226436630893\n",
      "epoch 9 loss: 1.0489028055513103\n",
      "epoch 10 loss: 1.0302425057894742\n",
      "epoch 11 loss: 1.009692132566416\n",
      "epoch 12 loss: 0.9871985164770469\n",
      "epoch 13 loss: 0.9628142195149852\n",
      "epoch 14 loss: 0.9367129358854356\n",
      "epoch 15 loss: 0.9092019589102255\n",
      "epoch 16 loss: 0.8807506381121777\n",
      "epoch 17 loss: 0.8518976236237925\n",
      "epoch 18 loss: 0.8231945808305751\n",
      "epoch 19 loss: 0.7951792431315988\n",
      "epoch 20 loss: 0.7685188090175825\n",
      "epoch 21 loss: 0.7436526180572713\n",
      "epoch 22 loss: 0.7210916051706865\n",
      "epoch 23 loss: 0.7011346031413808\n",
      "epoch 24 loss: 0.6837963650172062\n",
      "epoch 25 loss: 0.668689452594289\n",
      "epoch 26 loss: 0.6555806664618568\n",
      "epoch 27 loss: 0.644021619609268\n",
      "epoch 28 loss: 0.6338680984003184\n",
      "epoch 29 loss: 0.6247566724733117\n",
      "epoch 30 loss: 0.6170404915749043\n",
      "epoch 31 loss: 0.6104842450706305\n",
      "epoch 32 loss: 0.6071410695797063\n",
      "epoch 33 loss: 0.6088844129961404\n",
      "epoch 34 loss: 0.6233148371922603\n",
      "epoch 35 loss: 0.6054111209383225\n",
      "epoch 36 loss: 0.6288758809141123\n",
      "epoch 37 loss: 0.5836763232355884\n",
      "epoch 38 loss: 0.5909975218873186\n",
      "epoch 39 loss: 0.5812273254675522\n",
      "epoch 40 loss: 0.5992065957839593\n",
      "epoch 41 loss: 0.5703957356800688\n",
      "epoch 42 loss: 0.5850314819170125\n",
      "epoch 43 loss: 0.5641587772577592\n",
      "epoch 44 loss: 0.5816301721224499\n",
      "epoch 45 loss: 0.5554033265149911\n",
      "epoch 46 loss: 0.5689664083614046\n",
      "epoch 47 loss: 0.5506681787993389\n",
      "epoch 48 loss: 0.569229842352733\n",
      "epoch 49 loss: 0.5416355251807679\n",
      "epoch 50 loss: 0.55727868204756\n",
      "epoch 51 loss: 0.5385728026219778\n",
      "epoch 52 loss: 0.5581036766257575\n",
      "epoch 53 loss: 0.5306773459809923\n",
      "epoch 54 loss: 0.5500695580548634\n",
      "epoch 55 loss: 0.5287260082285128\n",
      "epoch 56 loss: 0.5532255660071334\n",
      "epoch 57 loss: 0.5198364135013913\n",
      "epoch 58 loss: 0.5365682427634174\n",
      "epoch 59 loss: 0.5184010726908652\n",
      "epoch 60 loss: 0.5390588364660298\n",
      "epoch 61 loss: 0.512452981395508\n",
      "epoch 62 loss: 0.5307439604754765\n",
      "epoch 63 loss: 0.5103268253822233\n",
      "epoch 64 loss: 0.5364459174622308\n",
      "epoch 65 loss: 0.5022590034657697\n",
      "epoch 66 loss: 0.5197421425134876\n",
      "epoch 67 loss: 0.5007153482330131\n",
      "epoch 68 loss: 0.5291790174192206\n",
      "epoch 69 loss: 0.49431104718342767\n",
      "epoch 70 loss: 0.5160038859137669\n",
      "epoch 71 loss: 0.49490692595250585\n",
      "epoch 72 loss: 0.5300065107194576\n",
      "epoch 73 loss: 0.4826859960972152\n",
      "epoch 74 loss: 0.5029288120725688\n",
      "epoch 75 loss: 0.48565180336519354\n",
      "epoch 76 loss: 0.523140156256916\n",
      "epoch 77 loss: 0.47497541398639265\n",
      "epoch 78 loss: 0.4966483927915688\n",
      "epoch 79 loss: 0.4784858473429766\n",
      "epoch 80 loss: 0.5185478085791848\n",
      "epoch 81 loss: 0.46743827677270994\n",
      "epoch 82 loss: 0.49039656610974114\n",
      "epoch 83 loss: 0.47153395446836005\n",
      "epoch 84 loss: 0.5166485797820751\n",
      "epoch 85 loss: 0.4551683918205044\n",
      "epoch 86 loss: 0.46215832863689743\n",
      "epoch 87 loss: 0.46342613608776656\n",
      "epoch 88 loss: 0.5075596098812643\n",
      "epoch 89 loss: 0.45015602408406724\n",
      "epoch 90 loss: 0.46317503401493926\n",
      "epoch 91 loss: 0.4616052451102998\n",
      "epoch 92 loss: 0.5006418105303624\n",
      "epoch 93 loss: 0.4566470301833629\n",
      "epoch 94 loss: 0.4449871512549896\n",
      "epoch 95 loss: 0.4480010063804297\n",
      "epoch 96 loss: 0.4904451177392062\n",
      "epoch 97 loss: 0.4437631595268682\n",
      "epoch 98 loss: 0.4789813048845837\n",
      "epoch 99 loss: 0.4492232937707389\n",
      "epoch 100 loss: 0.47925051086341963\n",
      "epoch 101 loss: 0.44742051071385824\n",
      "epoch 102 loss: 0.44275126044561486\n",
      "epoch 103 loss: 0.4414341358889832\n",
      "epoch 104 loss: 0.4929367862028769\n",
      "epoch 105 loss: 0.42841268697297497\n",
      "epoch 106 loss: 0.4266312090013669\n",
      "epoch 107 loss: 0.4300695053240035\n",
      "epoch 108 loss: 0.46894415573667886\n",
      "epoch 109 loss: 0.4359180024787277\n",
      "epoch 110 loss: 0.44508092502439706\n",
      "epoch 111 loss: 0.4430522733957549\n",
      "epoch 112 loss: 0.4962273368902871\n",
      "epoch 113 loss: 0.4307838109379935\n",
      "epoch 114 loss: 0.40864452254248734\n",
      "epoch 115 loss: 0.4177364011801387\n",
      "epoch 116 loss: 0.4829071235749416\n",
      "epoch 117 loss: 0.40497692345010583\n",
      "epoch 118 loss: 0.4312481352904382\n",
      "epoch 119 loss: 0.4211261204289986\n",
      "epoch 120 loss: 0.4905685733123074\n",
      "epoch 121 loss: 0.40456673628338363\n",
      "epoch 122 loss: 0.4030998569820781\n",
      "epoch 123 loss: 0.4035357811930311\n",
      "epoch 124 loss: 0.44631582454940216\n",
      "epoch 125 loss: 0.42296613405401007\n",
      "epoch 126 loss: 0.4632948016546053\n",
      "epoch 127 loss: 0.4007214342003483\n",
      "epoch 128 loss: 0.39458132236968607\n",
      "epoch 129 loss: 0.4033818349099089\n",
      "epoch 130 loss: 0.45895980924866236\n",
      "epoch 131 loss: 0.39971525691240434\n",
      "epoch 132 loss: 0.39959172560452944\n",
      "epoch 133 loss: 0.4120126065765308\n",
      "epoch 134 loss: 0.4735380530878409\n",
      "epoch 135 loss: 0.3840009837461247\n",
      "epoch 136 loss: 0.3652366160316005\n",
      "epoch 137 loss: 0.3726508513019873\n",
      "epoch 138 loss: 0.4113914481950689\n",
      "epoch 139 loss: 0.397819621693393\n",
      "epoch 140 loss: 0.4869480300771502\n",
      "epoch 141 loss: 0.36275021755304593\n",
      "epoch 142 loss: 0.3640225460540089\n",
      "epoch 143 loss: 0.37210719439447765\n",
      "epoch 144 loss: 0.43406740871406096\n",
      "epoch 145 loss: 0.4034931424260666\n",
      "epoch 146 loss: 0.5042922671437867\n",
      "epoch 147 loss: 0.3460387932744015\n",
      "epoch 148 loss: 0.34906780572459195\n",
      "epoch 149 loss: 0.382711303910926\n",
      "epoch 150 loss: 0.39532519610094724\n",
      "epoch 151 loss: 0.49513467952907975\n",
      "epoch 152 loss: 0.3366657817102485\n",
      "epoch 153 loss: 0.3381849873636099\n",
      "epoch 154 loss: 0.3599125356545926\n",
      "epoch 155 loss: 0.36822177962529196\n",
      "epoch 156 loss: 0.45376701071594494\n",
      "epoch 157 loss: 0.35100249090249896\n",
      "epoch 158 loss: 0.3866569729744773\n",
      "epoch 159 loss: 0.38841640112098097\n",
      "epoch 160 loss: 0.481033507520184\n",
      "epoch 161 loss: 0.3629141808271592\n",
      "epoch 162 loss: 0.3374649332629512\n",
      "epoch 163 loss: 0.34155434114291594\n",
      "epoch 164 loss: 0.398278488282082\n",
      "epoch 165 loss: 0.38030187576718805\n",
      "epoch 166 loss: 0.490709875967757\n",
      "epoch 167 loss: 0.3197585240444974\n",
      "epoch 168 loss: 0.32102206575174513\n",
      "epoch 169 loss: 0.3444237399023761\n",
      "epoch 170 loss: 0.35222605650122624\n",
      "epoch 171 loss: 0.4407607348436418\n",
      "epoch 172 loss: 0.33699567917648243\n",
      "epoch 173 loss: 0.3711624948721338\n",
      "epoch 174 loss: 0.3592905028389166\n",
      "epoch 175 loss: 0.4495797812019196\n",
      "epoch 176 loss: 0.33635095108440877\n",
      "epoch 177 loss: 0.3366592792194641\n",
      "epoch 178 loss: 0.3348979400976603\n",
      "epoch 179 loss: 0.3770210735699762\n",
      "epoch 180 loss: 0.36889305297012276\n",
      "epoch 181 loss: 0.43607402575394955\n",
      "epoch 182 loss: 0.3266541674045128\n",
      "epoch 183 loss: 0.3179127183686019\n",
      "epoch 184 loss: 0.32471162058408526\n",
      "epoch 185 loss: 0.37291133144667765\n",
      "epoch 186 loss: 0.3490463140744229\n",
      "epoch 187 loss: 0.43928212613778883\n",
      "epoch 188 loss: 0.32127550358132667\n",
      "epoch 189 loss: 0.34157744060646045\n",
      "epoch 190 loss: 0.33812865515322726\n",
      "epoch 191 loss: 0.41599078025031605\n",
      "epoch 192 loss: 0.3256412689754607\n",
      "epoch 193 loss: 0.3288783184463284\n",
      "epoch 194 loss: 0.3310582708567054\n",
      "epoch 195 loss: 0.3786447312634843\n",
      "epoch 196 loss: 0.3477322345859524\n",
      "epoch 197 loss: 0.37516876620794837\n",
      "epoch 198 loss: 0.34879470901661275\n",
      "epoch 199 loss: 0.3918453048653016\n",
      "epoch 200 loss: 0.3557455323778963\n",
      "epoch 201 loss: 0.38856983546616053\n",
      "epoch 202 loss: 0.34755059005448685\n",
      "epoch 203 loss: 0.3619425442161729\n",
      "epoch 204 loss: 0.3572306034576593\n",
      "epoch 205 loss: 0.3968952209919672\n",
      "epoch 206 loss: 0.3421826799689149\n",
      "epoch 207 loss: 0.34510535233951634\n",
      "epoch 208 loss: 0.3371827512481101\n",
      "epoch 209 loss: 0.37401399131669905\n",
      "epoch 210 loss: 0.3433810146388282\n",
      "epoch 211 loss: 0.37914497292584026\n",
      "epoch 212 loss: 0.326137176103929\n",
      "epoch 213 loss: 0.3612202774337214\n",
      "epoch 214 loss: 0.33975494205804374\n",
      "epoch 215 loss: 0.3874954068121363\n",
      "epoch 216 loss: 0.33308630508172465\n",
      "epoch 217 loss: 0.3518026760443297\n",
      "epoch 218 loss: 0.3202059459726495\n",
      "epoch 219 loss: 0.3896102128439837\n",
      "epoch 220 loss: 0.3085281762465131\n",
      "epoch 221 loss: 0.34844543372849607\n",
      "epoch 222 loss: 0.3211571126098143\n",
      "epoch 223 loss: 0.3797105380557608\n",
      "epoch 224 loss: 0.30129687401667155\n",
      "epoch 225 loss: 0.34208164706993777\n",
      "epoch 226 loss: 0.3152944108638737\n",
      "epoch 227 loss: 0.3787094280682882\n",
      "epoch 228 loss: 0.29784619810056817\n",
      "epoch 229 loss: 0.3328773697730776\n",
      "epoch 230 loss: 0.30311940709728685\n",
      "epoch 231 loss: 0.3497920807035174\n",
      "epoch 232 loss: 0.2945525199214055\n",
      "epoch 233 loss: 0.34698740435972214\n",
      "epoch 234 loss: 0.29413851717241896\n",
      "epoch 235 loss: 0.3430995681183885\n",
      "epoch 236 loss: 0.28954215026306046\n",
      "epoch 237 loss: 0.35275914577172623\n",
      "epoch 238 loss: 0.28741288650408614\n",
      "epoch 239 loss: 0.3506803525921112\n",
      "epoch 240 loss: 0.28923147690538914\n",
      "epoch 241 loss: 0.35836854025164006\n",
      "epoch 242 loss: 0.28726284108884553\n",
      "epoch 243 loss: 0.3536968659485372\n",
      "epoch 244 loss: 0.28813858485554605\n",
      "epoch 245 loss: 0.350361400169399\n",
      "epoch 246 loss: 0.2787100482971932\n",
      "epoch 247 loss: 0.33911597663946735\n",
      "epoch 248 loss: 0.27427424232943387\n",
      "epoch 249 loss: 0.3292900435313611\n",
      "epoch 250 loss: 0.27989534579989633\n",
      "epoch 251 loss: 0.3389786000559531\n",
      "epoch 252 loss: 0.2724852137200241\n",
      "epoch 253 loss: 0.3284405465578841\n",
      "epoch 254 loss: 0.27514918569489005\n",
      "epoch 255 loss: 0.3302582803345553\n",
      "epoch 256 loss: 0.2629539084882451\n",
      "epoch 257 loss: 0.31027713391076356\n",
      "epoch 258 loss: 0.26319899390204343\n",
      "epoch 259 loss: 0.2974990806823036\n",
      "epoch 260 loss: 0.27304524994836094\n",
      "epoch 261 loss: 0.32824398684149153\n",
      "epoch 262 loss: 0.26561458939603677\n",
      "epoch 263 loss: 0.3206757192284915\n",
      "epoch 264 loss: 0.25759635538693343\n",
      "epoch 265 loss: 0.297545150873896\n",
      "epoch 266 loss: 0.2554374062112054\n",
      "epoch 267 loss: 0.29879834634857194\n",
      "epoch 268 loss: 0.2553924642581913\n",
      "epoch 269 loss: 0.29644031234571505\n",
      "epoch 270 loss: 0.2538017533225501\n",
      "epoch 271 loss: 0.2931435730943622\n",
      "epoch 272 loss: 0.2516477834464458\n",
      "epoch 273 loss: 0.2908062263144051\n",
      "epoch 274 loss: 0.25443113025615216\n",
      "epoch 275 loss: 0.29557711417686866\n",
      "epoch 276 loss: 0.2505490189243776\n",
      "epoch 277 loss: 0.2873597588829514\n",
      "epoch 278 loss: 0.24875880637575218\n",
      "epoch 279 loss: 0.27787338046724946\n",
      "epoch 280 loss: 0.2435273451602285\n",
      "epoch 281 loss: 0.26746088796559525\n",
      "epoch 282 loss: 0.2548991459418128\n",
      "epoch 283 loss: 0.3068482679172578\n",
      "epoch 284 loss: 0.2482801626868729\n",
      "epoch 285 loss: 0.27986761653539777\n",
      "epoch 286 loss: 0.2405789584627037\n",
      "epoch 287 loss: 0.27598894164665444\n",
      "epoch 288 loss: 0.24058101897658699\n",
      "epoch 289 loss: 0.26909880792681096\n",
      "epoch 290 loss: 0.24882117136347953\n",
      "epoch 291 loss: 0.2947555708973411\n",
      "epoch 292 loss: 0.2374201246118383\n",
      "epoch 293 loss: 0.26434001850560807\n",
      "epoch 294 loss: 0.24267274491760502\n",
      "epoch 295 loss: 0.2860444345953944\n",
      "epoch 296 loss: 0.23158775717682076\n",
      "epoch 297 loss: 0.25079290267499954\n",
      "epoch 298 loss: 0.23402132049675525\n",
      "epoch 299 loss: 0.26897175007511465\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "32019f4a312570c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-07T13:37:58.931835Z",
     "start_time": "2025-02-07T13:37:58.924329Z"
    }
   },
   "source": [
    "accuracy = np.sum(np.argmax(model.predict(test_x), axis=1) == np.argmax(test_y, axis=1)) / test_x.shape[0]\n",
    "\n",
    "print(f'Accuracy: {accuracy}')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9\n"
     ]
    }
   ],
   "execution_count": 6
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
